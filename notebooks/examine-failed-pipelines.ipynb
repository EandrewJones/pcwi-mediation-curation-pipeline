{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "import time\n",
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "formatter = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data transformation utility classes\n",
    "#\n",
    "\n",
    "\n",
    "class ContentParser:\n",
    "    def __init__(self, xml_str):\n",
    "        self.tree = etree.fromstring(xml_str)\n",
    "        self.nsmap = self.get_nsmap()\n",
    "        # Parse\n",
    "        self.title = self.get_title()\n",
    "        self.author = self.get_author()\n",
    "        self.publication_date = self.get_published()\n",
    "        self.body_text = self.get_body_text()\n",
    "        self.lang = self.get_language()\n",
    "\n",
    "    def get_nsmap(self):\n",
    "        nsmap = {}\n",
    "        for ns in self.tree.xpath(\"//namespace::*\"):\n",
    "            if not ns[0] and ns[1] != \"\":\n",
    "                nsmap[\"atom\"] = ns[1]\n",
    "            elif ns[0]:\n",
    "                nsmap[ns[0]] = ns[1]\n",
    "        return nsmap\n",
    "\n",
    "    def get_title(self):\n",
    "        title = self.tree.find(\"atom:title\", namespaces=self.nsmap).text\n",
    "        if not title:\n",
    "            title_list = self.tree.xpath(\".//nitf:hl1\", namespaces=self.nsmap)\n",
    "            title = title_list[0].text if len(title_list) > 0 else None\n",
    "        return title\n",
    "\n",
    "    def get_author(self):\n",
    "        if self.tree.find(\".//author/person/nameText\") is not None:\n",
    "            return self.tree.find(\".//author/person/nameText\").text\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_published(self):\n",
    "        return self.tree.find(\"atom:published\", namespaces=self.nsmap).text\n",
    "\n",
    "    def get_body_text(self):\n",
    "        text = None\n",
    "        try:\n",
    "            text = \"\\n\\n\".join(\n",
    "                self.tree.find(\"atom:content/articleDoc\", namespaces=self.nsmap)\n",
    "                .find(\".//bodyText\")\n",
    "                .itertext()\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parser failed to parse document content: {e}\")\n",
    "        return text\n",
    "\n",
    "    def get_content_map(self):\n",
    "        return {\n",
    "            \"title\": self.title,\n",
    "            \"author\": self.author,\n",
    "            \"publication_date\": self.publication_date,\n",
    "            \"text\": self.body_text,\n",
    "            \"lang\": self.lang,\n",
    "        }\n",
    "\n",
    "    def get_language(self):\n",
    "        lang_list = self.tree.find(\"atom:content/articleDoc\", namespaces=self.nsmap).xpath(\n",
    "            \"./@xml:lang\"\n",
    "        )\n",
    "        return lang_list[0] if len(lang_list) > 0 else 'en'\n",
    "\n",
    "\n",
    "def parse_content(xml):\n",
    "    content = ContentParser(xml_str=xml)\n",
    "    return content.get_content_map()\n",
    "\n",
    "\n",
    "#\n",
    "# Helper functions\n",
    "#\n",
    "\n",
    "\n",
    "def json_to_dataframe(json_list, cols=None):\n",
    "    \"\"\"\n",
    "    Converts json list of fetched results into dataframe and subsets to columns of interest\n",
    "\n",
    "    params\n",
    "    ------\n",
    "    json_list:   list of dicts (JSON), data fetched from NU api\n",
    "    cols:   optional, list of column names to subset returned dataframe on\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    pandas dataframe\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting json list to Pandas DataFrame...\")\n",
    "    df = pd.json_normalize(json_list)\n",
    "    if cols:\n",
    "        df = df.filter(items=cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_results_df(results_df):\n",
    "    \"\"\"\n",
    "    Extracts the body text from news articles in API search results\n",
    "    \"\"\"\n",
    "    # Parse XML and append to df\n",
    "    logger.info(\"Parsing results...\")\n",
    "    doc_cont_parsed = \"Document.Content.Parsed\"\n",
    "    results_df[doc_cont_parsed] = results_df[\"Document.Content\"].apply(parse_content)\n",
    "    results_df = results_df.join(pd.json_normalize(results_df[doc_cont_parsed]))\n",
    "    results_df.drop(columns=doc_cont_parsed, inplace=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def clean_lang_field_and_subset(results_df, lang=\"en\"):\n",
    "    logger.info(f\"Cleaning language field and subsetting df to language: {lang} ...\")\n",
    "    results_df[\"lang\"] = results_df[\"lang\"].str.lower()\n",
    "    en_idx = results_df[\"lang\"] == lang  # Subset to desired language\n",
    "    return results_df.loc[en_idx]\n",
    "\n",
    "\n",
    "def clean_doc_ids(results_df):\n",
    "    logger.info(\"Cleaning document ids...\")\n",
    "    results_df[\"Document.DocumentId\"] = (\n",
    "        results_df[\"Document.DocumentId\"].str.split(\"contentItem:\").str[1]\n",
    "    )\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def drop_null_rows(df):\n",
    "    n_rows = df[df[\"Document.Content\"].isnull()].shape[0]\n",
    "    logger.info(f\"Dropping {n_rows} rows with missing content...\")\n",
    "    return df[~df[\"Document.Content\"].isnull()]\n",
    "\n",
    "\n",
    "def drop_missing_text(df):\n",
    "    n_rows = df[df[\"text\"].isnull()].shape[0]\n",
    "    logger.info(f\"Dropping {n_rows} rows with missing text after parsing...\")\n",
    "    return df[~df[\"text\"].isnull()]\n",
    "\n",
    "\n",
    "def drop_unnecessary_cols(df, cols=[\"Document.Content\", \"author\", \"lang\"]):\n",
    "    logger.info(\"Removing unneeded columns...\")\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "def sort_by_date(df, asc=True):\n",
    "    logger.info(f\"Sorting documents by date, {'ascending' if asc else 'descending'}...\")\n",
    "    df[\"publication_date\"] = pd.to_datetime(df[\"publication_date\"])\n",
    "    return df.sort_values(by=\"publication_date\", ascending=asc)\n",
    "\n",
    "\n",
    "def transform_pipeline(json_list):\n",
    "    \"\"\"\n",
    "    Pipeline of transformations to perform on the fetched data\n",
    "    \"\"\"\n",
    "    df = json_to_dataframe(\n",
    "        json_list=json_list,\n",
    "        cols=[\"ResultID\", \"Document.DocumentId\", \"Document.Content\", \"Source.Name\"],\n",
    "    )\n",
    "    df = clean_doc_ids(results_df=df)\n",
    "    df = drop_null_rows(df=df)\n",
    "    df = parse_results_df(results_df=df)\n",
    "    df = clean_lang_field_and_subset(results_df=df)\n",
    "    df = drop_unnecessary_cols(df=df)\n",
    "    df = sort_by_date(df=df)\n",
    "    df = drop_missing_text(df=df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_json_list(path):\n",
    "    logger.info(f\"Loading file: {path}\")\n",
    "    with open(path, \"rb\") as infile:\n",
    "        json_list = json.load(infile)\n",
    "    return json_list\n",
    "\n",
    "\n",
    "def df_to_json(path, df):\n",
    "    logger.info(f\"Writing dataframe to JSON {path}...\")\n",
    "    try:\n",
    "        df.to_json(path, orient=\"records\", lines=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Encountered error {e} when writing JSON\")\n",
    "        raise e\n",
    "    logger.info(\"JSON saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Retrieving raaw data for transforms\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Retrieving raaw data for transforms\")\n",
    "transformed = [f for f in os.listdir(\"data/dispute-dataframes/\")]\n",
    "files = [f for f in os.listdir(\"data/nu-api-data-raw\") if f not in transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'Canada-QuebecoisORQuebecerORQuebecker?Date_gt_1991-01-01_and_Date_lt_2015-12-31.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Loading dataset: Canada-QuebecoisORQuebecerORQuebecker?Date_gt_1991-01-01_and_Date_lt_2015-12-31.json\n",
      "__main__ - INFO - Loading file: data/nu-api-data-raw/Canada-QuebecoisORQuebecerORQuebecker?Date_gt_1991-01-01_and_Date_lt_2015-12-31.json\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading dataset: {f}\")\n",
    "json_list = load_json_list(path=os.path.join(\"data/nu-api-data-raw\", f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Converting json list to Pandas DataFrame...\n",
      "__main__ - INFO - Cleaning document ids...\n",
      "__main__ - INFO - Dropping 4 rows with missing content...\n"
     ]
    }
   ],
   "source": [
    "df = json_to_dataframe(\n",
    "        json_list=json_list,\n",
    "        cols=[\"ResultID\", \"Document.DocumentId\", \"Document.Content\", \"Source.Name\"],\n",
    "    )\n",
    "df = clean_doc_ids(results_df=df)\n",
    "df = drop_null_rows(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n",
      "__main__ - ERROR - Parser failed to parse document content: 'NoneType' object has no attribute 'itertext'\n"
     ]
    }
   ],
   "source": [
    "problem_rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        content = ContentParser(xml_str=row['Document.Content'])\n",
    "    except:\n",
    "        problem_rows.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6637, 35970]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.fromstring(df['Document.Content'][6637])\n",
    "nsmap = {}\n",
    "for ns in tree.xpath(\"//namespace::*\"):\n",
    "    if not ns[0] and ns[1] != \"\":\n",
    "        nsmap[\"atom\"] = ns[1]\n",
    "    elif ns[0]:\n",
    "        nsmap[ns[0]] = ns[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.find(\"atom:content/articleDoc\", namespaces=nsmap).xpath(\n",
    "            \"./@xml:lang\"\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "XPathEvalError",
     "evalue": "Invalid expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXPathEvalError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85257/579867623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"atom:content/articleDoc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._Element.xpath\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree.XPathElementEvaluator.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree._XPathEvaluatorBase._handle_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mXPathEvalError\u001b[0m: Invalid expression"
     ]
    }
   ],
   "source": [
    "tree.find(\"atom:content/articleDoc\", namespaces=nsmap).xpath(\"./\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<entry xmlns=\"http://www.w3.org/2005/Atom\"><id>urn:contentItem:4HDN-TSM0-TWMB-51XD-00000-00</id><title>Rice dismisses \\'trade dispute\\' Issues between the two countries remain difficult</title><published>2005-10-26T00:00:00Z</published><updated>2022-07-22T09:21:20Z</updated><author><name>LexisNexis</name></author><content type=\"application/xml\"><!--Transformation version 1.25--><articleDoc xmlns=\"\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://www.lexisnexis.com/xmlschemas/content/public/articledoc/1/\" schemaVersion=\"1.8\"><articleDocHead><itemInfo><sourceSectionInfo><positionSection>NEWS</positionSection><positionSequence>Pg. A07</positionSequence></sourceSectionInfo></itemInfo></articleDocHead><nitf:body xmlns:nitf=\"http://iptc.org/std/NITF/2006-10-18/\"><nitf:body.head><nitf:hedline><nitf:hl1>Rice dismisses \\'trade dispute\\' Issues between the two countries remain difficult</nitf:hl1></nitf:hedline><nitf:byline><author><person><nameText>Graham Fraser</nameText></person></author><nitf:byttl>Toronto Star</nitf:byttl></nitf:byline><nitf:dateline><location>OTTAWA</location></nitf:dateline></nitf:body.head><nitf:body.content><bodyText><p nitf:lede=\"true\">The global issues were easy during U.S. Secretary of State Condoleezza Rice\\'s two-day visit to Ottawa - but the bilateral issues remain intractable.</p><p nitf:lede=\"true\">Rice praised Canada\\'s role in Haiti yesterday, expressed the hope that Canada would do more in Iraq if asked by the Iraqi government, and made it clear that she had discussed Iran, Syria, the threat of an avian flu pandemic and the upcoming Summit of the Americas in Argentina with Prime Minister Paul Martin and Foreign Affairs Minister Pierre Pettigrew.</p><p nitf:lede=\"true\">\"The new Iraqi government, once it is elected, will have plans. It will have reconstruction plans,\" she said, adding that the United States has carried most of the load in rebuilding Iraq.</p><p>\"I think that Canada, which has been generous, will, I hope and believe, look at what more can be done to support the Iraqi government once there\\'s a new government in place.\"</p><p>But she dismissed the contentious softwood lumber issue as \"a trade dispute,\" insisting that it should be settled through further negotiations.</p><p>Rice bristled when asked how the U.S. could be trusted when it doesn\\'t live up to its international agreements.</p><p>\"Well, I think the word of the United States has been as good as gold in its international dealings and its agreements,\" she snapped.</p><p>Rice stressed that this is a trade dispute on a particular issue and that she would like to see negotiation of the issue succeed.</p><p>\"But I think it is extremely important not to speak in apocalyptic language about this issue,\" she said.</p><p>\"It is an important issue, but it is a trade dispute.\"</p><p>In the House of Commons, Conservative Leader Stephen Harper accused Prime Minister Paul Martin of talking tough with the Americans in public but being \"soft as putty\" in private.</p><p>Martin reiterated the Canadian position that after the latest NAFTA panel decision, the United States owes Canada $3.5 billion.</p><p>\"We will eventually win the other $1.5 billion and we will not negotiate a win,\" he said.</p><p>\"We will not negotiate unless we have signs that in fact NAFTA is being respected.\"</p><p>He was referring to the $5 billion raised in import duties imposed by Congress on Canadian softwood lumber - an imposition that was dismissed by a NAFTA panel.</p><p>International Trade Minister Jim Peterson said that Canada is pursuing the case before the courts, and he later told reporters that Canada is before the World Trade Organization seeking the right to retaliate.</p><p>Bloc Quebecois Leader Gilles Duceppe told reporters that Rice had said the same thing in her private meeting with the three opposition leaders that she did in public.</p><p>\"The Americans are consistent,\" he said.</p><p>\"They maintain the legal process as long as possible, hoping to exhaust the Canadian companies.</p><p>\"That\\'s their strategy, and they\\'re using NAFTA to do it.\"</p><p>He said that the U.S. would continue to follow its strategy if Canada doesn\\'t react.</p><p>Rice was supposed to visit Ottawa in March, shortly after being sworn in as the top diplomat in the U.S. administration.</p><p>But that visit was abruptly cancelled after Canada backed out of participation in the U.S. missile defence program.</p><p>At the news conference yesterday, Rice paid tribute to Rosa Parks, who died on Monday at the age of 92.</p><p>Parks became an internationally known figure when she refused to give up her seat to a white man on a segregated bus in Montgomery, Alabama in 1955.</p><p>Rice called her \"the pioneer in the civil rights movement who one day was just sick and tired of being sick and tired and refused to give up her seat and inspired a whole generation of people to fight for freedom.\"</p><p>She said that Parks had a life that was inspirational well beyond the single act on a Montgomery bus 50 years ago.</p><p>\"I think for all of us her inspiration will live on and I just wanted to acknowledge that,\" she said.</p><p>Rice had her own brush with the civil rights movement, and the segregationist backlash against it.</p><p>As a child in Birmingham, she was within earshot of the 1963 bombing of the 16th Street Baptist Church, and her neighbourhood friend Denise McNair was killed in that bombing.</p></bodyText></nitf:body.content><nitf:body.end><graphic><nitf:media><caption><p> Tom Hanson CP photo U.S. Secretary of State Condoleezza Rice and Governor General Michaelle Jean share a laugh prior to their meeting in Ottawa yesterday. Rice was in Ottawa for a two-day visit, during which she met with Prime Minister Paul Martin and Foreign Affairs Minister Pierre Pettigrew.</p></caption></nitf:media></graphic></nitf:body.end></nitf:body><metadata><dc:metadata xmlns:dc=\"http://purl.org/dc/elements/1.1/\"><dc:identifier identifierScheme=\"DOC-ID\">TSTAR-20051026449479</dc:identifier><dc:source sourceScheme=\"royaltyReportingKey\">TSTAR2005</dc:source><dc:identifier identifierScheme=\"PGUID\">urn:contentItem:4HDN-TSM0-TWMB-51XD-00000-00</dc:identifier><dc:source sourceScheme=\"productContentSetIdentifier\">8286</dc:source><dc:date dateType=\"last-updated\">2020-06-12</dc:date></dc:metadata><wordCount number=\"669\"/><publicationInfo><copyright>Copyright 2005 Toronto Star Newspapers, Ltd.</copyright><publicationName>The Toronto Star</publicationName><publicationDate day=\"26\" month=\"10\" year=\"2005\"><dateText>October 26, 2005 Wednesday</dateText></publicationDate><classification classificationScheme=\"publicationtype\"><classificationItem><className>NEWSPAPER</className></classificationItem></classification><classification classificationScheme=\"publicationtype\"><classificationItem><className>Newspapers</className></classificationItem></classification></publicationInfo><classification classificationScheme=\"language\"><classificationItem classificationScheme=\"lang.english\"><className>ENGLISH</className></classificationItem></classification><classification classificationScheme=\"typeofdoc\"><classificationItem><className>COLUMN</className></classificationItem></classification><classificationGroup classificationScheme=\"indexing-terms\"><classification classificationScheme=\"subject\"><classificationItem score=\"92\"><classCode>STX000950</classCode><className>INTERNATIONAL RELATIONS</className></classificationItem><classificationItem score=\"90\"><classCode>ST000DH0Q</classCode><className>AGREEMENTS</className></classificationItem><classificationItem score=\"90\"><classCode>STX000454</classCode><className>DEATH NOTICES &amp; OBITUARIES</className></classificationItem><classificationItem score=\"90\"><classCode>STX000752</classCode><className>FOREIGN POLICY</className></classificationItem><classificationItem score=\"90\"><classCode>ST000DLOJ</classCode><className>GOVERNMENT ADVISORS &amp; MINISTERS</className></classificationItem><classificationItem score=\"90\"><classCode>N928120MM</classCode><className>STATE DEPARTMENTS &amp; FOREIGN SERVICES</className></classificationItem><classificationItem score=\"89\"><classCode>ST00095A0</classCode><className>AGRICULTURAL TRADE</className></classificationItem><classificationItem score=\"89\"><classCode>ST000988Z</classCode><className>FREE TRADE TREATIES &amp; AGREEMENTS</className></classificationItem><classificationItem score=\"89\"><classCode>N920000CC</classCode><className>GOVERNMENT &amp; PUBLIC ADMINISTRATION</className></classificationItem><classificationItem score=\"89\"><classCode>N921110CC</classCode><className>HEADS OF STATE &amp; GOVERNMENT</className></classificationItem><classificationItem score=\"89\"><classCode>STX000951</classCode><className>INTERNATIONAL TRADE</className></classificationItem><classificationItem score=\"89\"><classCode>ST0009EOQ</classCode><className>PRIME MINISTERS</className></classificationItem><classificationItem score=\"89\"><classCode>STX001742</classCode><className>TRADE DISPUTES</className></classificationItem><classificationItem score=\"89\"><classCode>STX001758</classCode><className>TREATIES &amp; AGREEMENTS</className></classificationItem><classificationItem score=\"85\"><classCode>ST0008Z3C</classCode><className>EXPORT &amp; IMPORT LAW</className></classificationItem><classificationItem score=\"78\"><classCode>ST00097CC</classCode><className>COMMERCE DEPARTMENTS</className></classificationItem><classificationItem score=\"78\"><classCode>ST00098A0</classCode><className>FOREIGN RELATIONS</className></classificationItem><classificationItem score=\"78\"><classCode>N928000CC</classCode><className>INTERNATIONAL RELATIONS &amp; NATIONAL SECURITY</className></classificationItem><classificationItem score=\"78\"><classCode>N921120CC</classCode><className>LEGISLATIVE BODIES</className></classificationItem><classificationItem score=\"78\"><classCode>STX001669</classCode><className>TALKS &amp; MEETINGS</className></classificationItem><classificationItem score=\"77\"><classCode>ST0008ZBX</classCode><className>INTERNATIONAL LAW</className></classificationItem><classificationItem score=\"73\"><classCode>STX000895</classCode><className>IMPORT TRADE</className></classificationItem><classificationItem score=\"72\"><classCode>N813940MM</classCode><className>POLITICAL PARTIES</className></classificationItem><classificationItem score=\"71\"><classCode>ST0009FJL</classCode><className>AVIAN INFLUENZA</className></classificationItem><classificationItem score=\"70\"><classCode>ST0008WWF</classCode><className>POSTWAR RECONSTRUCTION</className></classificationItem><classificationItem score=\"64\"><classCode>STX001673</classCode><className>TARIFFS &amp; DUTIES</className></classificationItem><classificationItem score=\"60\"><classCode>ST000A7IS</classCode><className>ASSOCIATIONS &amp; ORGANIZATIONS</className></classificationItem><classificationItem score=\"60\"><classCode>ST0008XMR</classCode><className>LAW &amp; LEGAL SYSTEM</className></classificationItem><classificationItem score=\"56\"><classCode>STX002064</classCode><className>EPIDEMICS</className></classificationItem><classificationItem score=\"56\"><classCode>STX000914</classCode><className>INFLUENZA</className></classificationItem><classificationItem score=\"56\"><classCode>ST000DOXW</classCode><className>PANDEMICS</className></classificationItem></classification><classification classificationScheme=\"industry\"><classificationItem score=\"89\"><classCode>ST00095A0</classCode><className>AGRICULTURAL TRADE</className></classificationItem><classificationItem score=\"71\"><classCode>ST0009FJL</classCode><className>AVIAN INFLUENZA</className></classificationItem></classification><classification classificationScheme=\"person\"><classificationItem score=\"79\"><classCode>PE0009U1C</classCode><className>CONDOLEEZZA RICE</className></classificationItem><classificationItem score=\"79\"><classCode>PE0009UEP</classCode><className>PAUL MARTIN</className></classificationItem><classificationItem score=\"79\"><classCode>PE0009UEN</classCode><className>STEPHEN HARPER</className></classificationItem></classification><classification classificationScheme=\"city\"><classificationItem score=\"89\"><classCode>GX052</classCode><className>OTTAWA, ON, CANADA</className></classificationItem></classification><classification classificationScheme=\"state\"><classificationItem score=\"58\"><classCode>GS176</classCode><className>QUEBEC, CANADA</className></classificationItem></classification><classification classificationScheme=\"country\"><classificationItem score=\"97\"><classCode>GC339</classCode><className>CANADA</className></classificationItem><classificationItem score=\"95\"><classCode>GC343</classCode><className>UNITED STATES</className></classificationItem><classificationItem score=\"94\"><classCode>GC358</classCode><className>IRAQ</className></classificationItem><classificationItem score=\"79\"><classCode>GC337</classCode><className>ARGENTINA</className></classificationItem><classificationItem score=\"79\"><classCode>GC365</classCode><className>HAITI</className></classificationItem><classificationItem score=\"79\"><classCode>GC368</classCode><className>IRAN, ISLAMIC REPUBLIC OF</className></classificationItem><classificationItem score=\"79\"><classCode>GC363</classCode><className>SYRIA</className></classificationItem></classification></classificationGroup></metadata></articleDoc></content></entry>\\r\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Document.Content'][6637]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84c6cf241e2d4363798c1d5bb2c6abf91ff57ecee7c0f398991192bd5ac0efa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
